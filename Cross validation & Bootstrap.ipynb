{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f958749-3085-43b4-8a23-13cf5af54e83",
   "metadata": {},
   "source": [
    "<H2>CHAPTER 5 - Lab: Cross-Validation and the Bootstrap</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ca454-125d-4def-9e91-d2715d02fe97",
   "metadata": {},
   "source": [
    "> This section explores the resampling techniques covered in this chapter. Some of the commands may take a considerable amount of time to run on your computer. We begin by organizing most of the required imports at the top level.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac837e4-9324-4729-adb5-21c2e0fc534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "summarize,\n",
    "poly)\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314be59-2963-4976-a710-7a14a0d5ebfb",
   "metadata": {},
   "source": [
    "There are several new imports needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c552f7f-0857-480d-8f5d-5cf24d26754b",
   "metadata": {},
   "outputs": [],
   "source": [
    " from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "(cross_validate,\n",
    "KFold,\n",
    "ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49657756-af4f-4f78-a1ac-518e0f13e1ec",
   "metadata": {},
   "source": [
    "<h4>The Validation Set Approach</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a30e88-4f10-4ca1-b046-6de4f5b36583",
   "metadata": {},
   "source": [
    "The dataset is divided into training and validation subsets using the train_test_split() function. Since the dataset contains 392 observations, it is partitioned into two equally sized sets of 196 samples by specifying test_size=196. To ensure that the results are fully reproducible, it is good practice to fix the random seed when performing randomized operations. In this case, reproducibility is achieved by setting the random_state parameter to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be8d0ed7-5a6b-41fe-8736-78ff7834b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "test_size=196,\n",
    "random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f91c15-e5a7-4bda-adec-cd6105ea8ab1",
   "metadata": {},
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set Auto_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1963917-55a8-4840-adec-831f8fca9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ba027-bc8e-4a85-9fec-774cfd9832cf",
   "metadata": {},
   "source": [
    "The predict() method is applied to the model results using the model matrix constructed from the validation dataset. The validation mean squared error (MSE) is then computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a51e405-4a39-4b75-9ce6-64b4dd06b05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(23.616617069669882)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7dbad0-c0f3-4ee9-8fbe-35f061bd5b52",
   "metadata": {},
   "source": [
    "> Consequently, the estimated validation MSE for the linear regression model is 23.62. The validation error can also be evaluated for polynomial regression models of higher degree. To facilitate this, we first define a function, `evalMSE()`, which takes a model specification along with training and test datasets and returns the mean squared error computed on the test set.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2010706a-544d-4fe6-9c69-ee5d514ac454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMSE(terms,\n",
    "            response,\n",
    "            train,\n",
    "            test):\n",
    "    mm = MS(terms)\n",
    "    X_train = mm.fit_transform(train)\n",
    "    y_train = train[response]\n",
    "    X_test = mm.transform(test)\n",
    "    y_test = test[response]\n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    test_pred = results.predict(X_test)\n",
    "    return np.mean((y_test - test_pred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe7a51-3762-405e-ad64-b7ce15b5a2e4",
   "metadata": {},
   "source": [
    "This function is then used to estimate the validation MSE for linear, quadratic, and cubic models. The enumerate() function is employed to iterate through the models while simultaneously providing both the index and the corresponding value at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a81d7a20-5412-47ab-963f-e9bc83a56149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an array to store MSE for degrees 1, 2, 3\n",
    "MSE = np.zeros(3)\n",
    "\n",
    "# Loop through polynomial degrees 1, 2, 3\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "                        'mpg',\n",
    "                        Auto_train,\n",
    "                        Auto_valid)\n",
    "\n",
    "# Display the MSE array\n",
    "MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473483f-c54f-464f-ba8e-0bc5d0e3e592",
   "metadata": {},
   "source": [
    "The resulting error rates are 23.62, 18.76, and 18.80, respectively. However, if a different training–validation split is selected instead…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee7d045-6a7e-440c-b51a-f759ba49e33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset into training and validation sets\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                          test_size=196,\n",
    "                                          random_state=3)\n",
    "\n",
    "# Initialize array to store MSE for polynomial degrees 1, 2, 3\n",
    "MSE = np.zeros(3)\n",
    "\n",
    "# Loop over polynomial degrees\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "                        'mpg',\n",
    "                        Auto_train,\n",
    "                        Auto_valid)\n",
    "\n",
    "# Display the MSE array\n",
    "MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727dfe6-4c46-4eb7-9adb-d87e95ab31c2",
   "metadata": {},
   "source": [
    "The sklearn_sm() class takes a statsmodels model as its first argument. It also accepts two optional parameters: model_str, which is used to define a model formula, and model_args, a dictionary containing additional arguments required during model fitting. For instance, when fitting a logistic regression model, the family parameter must be specified and is provided through model_args = {'family': sm.families.Binomial()}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccab1798-6836-4c73-837d-3b1277936a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929233)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " hp_model = sklearn_sm(sm.OLS,\n",
    "MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "X,\n",
    "Y,\n",
    "cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276d93d-1783-43c2-97b5-2fd8bdab640f",
   "metadata": {},
   "source": [
    "> The `cross_validate()` function takes as input an object that implements the `fit()`, `predict()`, and `score()` methods, along with a feature matrix (X) and a response variable (Y). An additional argument, `cv`, is used to specify the cross-validation strategy: providing an integer (K) results in (K)-fold cross-validation. In this case, the value is set equal to the total number of observations, yielding leave-one-out cross-validation (LOOCV)\r\n",
    "> The `cross_validate()` function returns a dictionary containing several outputs; here, we focus on the cross-validated test score, namely the mean squared error (MSE), which is estimated to be 24.\r\n",
    ">\r\n",
    "> This procedure can be extended to polynomial regression models of increasing complexity. To automate the process, a for loop is used to fit polynomial models of degrees 1 through 5, compute the corresponding cross-validation errors, and store them in the (i)th element of the vector `cv_error`. The loop variable `d` denotes the degree of the polynomial. The vector is initialized prior to the loop, and the computation may take a few seconds to complete.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c7fd2f0-4f69-49df-981d-2c11a81a1daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.4244303 , 19.0332262 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "Y = np.array(Auto['mpg'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "\n",
    "for i, d in enumerate(range(1, 6)):\n",
    "    # Design matrix for polynomial regression\n",
    "    X = np.column_stack([H**p for p in range(d+1)])\n",
    "    \n",
    "    # LOOCV with MSE\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=Auto.shape[0],                # LOOCV\n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          return_train_score=False)\n",
    "    \n",
    "    # Store MSE (convert negative to positive)\n",
    "    cv_error[i] = -np.mean(M_CV['test_score'])\n",
    "\n",
    "cv_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ddb96-b7dc-4288-88d9-51082567e00f",
   "metadata": {},
   "source": [
    "Earlier, we introduced the outer() method in combination with np.power(). The outer() method can be applied to any function that takes two arguments, such as add(), min(), or power(). It takes two arrays as input and produces a larger array in which the specified operation is applied to every possible pair of elements from the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccc8b32f-8e0b-4bac-9881-9e3cb1071af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  7],\n",
       "       [ 7,  9],\n",
       "       [11, 13]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " A = np.array([3, 5, 9])\n",
    "B = np.array([2, 4])\n",
    "np.add.outer(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4cf75-35fd-4903-a21b-1101b7a80a93",
   "metadata": {},
   "source": [
    "> In the CV example above, we set (K = n), but it is also possible to choose (K < n). The code is very similar to the previous example and runs significantly faster. Here, we use `KFold()` to divide the data into (K = 10) random folds. A random seed is set via `random_state` for reproducibility, and a vector `cv_error` is initialized to store the cross-validation errors for polynomial fits of degrees one through five.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb3e7600-3c60-47e6-bf5c-eb0cf92850db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13719075])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "\n",
    "# Use the same K-Fold splits for all degrees\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for i, d in enumerate(range(1, 6)):\n",
    "    # Create polynomial design matrix for degree d\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=cv)\n",
    "    \n",
    "    # Store mean test score\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "\n",
    "# Display cross-validation errors\n",
    "cv_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01accd70-8e55-4a17-a887-039ed0ecf104",
   "metadata": {},
   "source": [
    "> The `cross_validate()` function is flexible and allows different data-splitting strategies. For example, it can be used to implement the validation set approach just as easily as K-fold cross-validation.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e89f7c52-d7c4-411d-95c3-06edcf83f7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=1,\n",
    "test_size=196,\n",
    "random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "Auto.drop(['mpg'], axis=1),\n",
    "Auto['mpg'],\n",
    "cv=validation);\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3ca79-f078-4326-a4e7-7f095ff9c5d7",
   "metadata": {},
   "source": [
    "The variability in the test error can be estimated by executing this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "171d88c9-8753-4a47-881f-5cba5bcf8b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(23.802232661034164), np.float64(1.4218450941091862))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10,\n",
    "test_size=196,\n",
    "random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "Auto.drop(['mpg'], axis=1),\n",
    "Auto['mpg'],\n",
    "cv=validation)\n",
    "results['test_score'].mean(), results['test_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e0712-d8b2-4fc9-8432-62573c2c643c",
   "metadata": {},
   "source": [
    "<h4>Estimation of the accuracy of statistic of interest</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdef95-797c-49e4-aeb9-8fc45dc63027",
   "metadata": {},
   "source": [
    "> A function `alpha_func()` is created, which takes as input a DataFrame `D` containing columns `X` and `Y`, along with a vector `idx` specifying the observations used to estimate (\\alpha). The function then returns the estimate of (\\alpha) based on the selected observations.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52d8660e-84ba-4d07-87da-d14ce8e82428",
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio = load_data('Portfolio')\n",
    "\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
    "    return ( (cov_[1,1] - cov_[0,1]) / (cov_[0,0] + cov_[1,1] - 2*cov_[0,1]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9aa7f8-90cd-4b80-a829-02ab120cf792",
   "metadata": {},
   "source": [
    "> This function computes an estimate of (\\alpha) by applying the minimum variance formula (5.7) to the observations specified by the `idx` argument. For example, the command below estimates (\\alpha) using all 100 observations.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f21e7039-3bf7-4e9c-80c4-f8976890b384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.57583207459283)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fda8cb-d96b-401e-8321-7adbb0baffdf",
   "metadata": {},
   "source": [
    "> Next, we randomly select 100 observations from `range(100)` with replacement. This effectively creates a new bootstrap dataset and allows us to recompute (\\hat{\\alpha}) based on the resampled data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ff028fb-7243-4318-b5a3-d284ba2e6341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6074452469619004)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio,\n",
    "rng.choice(100,\n",
    "100,\n",
    "replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7795ccf-fea1-4fcd-a79b-bfa73ca27c37",
   "metadata": {},
   "source": [
    "> This method can be extended to define a simple function, `boot_SE()`, which computes the bootstrap standard error for any function that takes a DataFrame as input.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1287ce81-3239-4ebc-affc-f2b6c7d98900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_SE(func,\n",
    "            D,\n",
    "            n=None,\n",
    "            B=1000,\n",
    "            seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index,\n",
    "                         n,\n",
    "                         replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b718d6-925d-42c4-bc1a-8b475bbf8365",
   "metadata": {},
   "source": [
    "> Note the use of `_` as the loop variable in `for _ in range(B)`. This is commonly done when the loop counter itself is not needed and the purpose is simply to execute the loop `B` times. We can now use our function to assess the accuracy of our (\\alpha) estimate with `B = 1,000` bootstrap replications.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc2dca5f-b426-4650-b0bd-5c4721928f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09118176521277699)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_SE = boot_SE(alpha_func,\n",
    "Portfolio,\n",
    "B=1000,\n",
    "seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4a062-024f-4af5-9931-b79058bc401a",
   "metadata": {},
   "source": [
    "<h4>Estimating the Accuracy of a Linear Regression Model</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61b766-1628-43ec-a6a6-be473db364c6",
   "metadata": {},
   "source": [
    "> We begin by defining a generic function `boot_OLS()` for bootstrapping a regression model, where the regression is specified using a formula. The `clone()` function is used to create a copy of the formula that can be refit to the resampled DataFrame. This ensures that any derived features, such as those created with `poly()` (which will be demonstrated shortly), are recalculated on the bootstrap sample.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7a841df1-cb2a-41c5-981d-71f02223adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.iloc[idx]  # Use iloc to index by integer positions\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f32684a-1a90-4f00-b24e-60e7b21285f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6297f1a9-31f2-4fd5-8581-155d519d9c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.88064456, -0.1567849 ],\n",
       "       [38.73298691, -0.14699495],\n",
       "       [38.31734657, -0.14442683],\n",
       "       [39.91446826, -0.15782234],\n",
       "       [39.43349349, -0.15072702],\n",
       "       [40.36629857, -0.15912217],\n",
       "       [39.62334517, -0.15449117],\n",
       "       [39.0580588 , -0.14952908],\n",
       "       [38.66688437, -0.14521037],\n",
       "       [39.64280792, -0.15555698]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "np.array([\n",
    "    hp_func(Auto, rng.choice(Auto.shape[0], Auto.shape[0], replace=True))\n",
    "    for _ in range(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff23d1a-478c-493c-b631-0fb11ab15cbb",
   "metadata": {},
   "source": [
    "Then w We apply the `boot_SE()` function to calculate the standard errors of 1,000 bootstrap estimates for both the intercept and slope coefficients.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05f7fe6e-63bd-4c00-a954-9c1026fe36df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.731176\n",
       "horsepower    0.006092\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_se = boot_SE(hp_func,\n",
    "Auto,\n",
    "B=1000,\n",
    "seed=10)\n",
    "hp_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c46a29-63da-48b8-bb53-081dc762f746",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "This shows that the bootstrap estimate of SE(β̂₀) is 0.85, while the bootstrap estimate of SE(β̂₁) is 0.0074. As explained in Section 3.1.2, standard formulas can also be used to calculate the standard errors of the regression coefficients in a linear model. These values can be obtained using the `summarize()` function from **ISLP.smo that?\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "77329503-1820-4584-97e8-7df4b600e9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717\n",
       "horsepower    0.006\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto['mpg'])\n",
    "model_se = summarize(hp_model.results_)['std err']\n",
    "model_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad3e7ab-e8b2-4bb5-86ec-aeed317d2f11",
   "metadata": {},
   "source": [
    "Here b\n",
    "\r\n",
    "Below, we calculate both the bootstrap standard error estimates and the standard linear regression estimates for the quadratic model fitted to the data. Because this model fits the data well (Figure 3.8), the bootstrap estimates now align more closely with the standard estimates of SE(β̂₀), SE(β̂₁), and SE(β̂o that?\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e13f4884-0b33-4dd9-bba3-cbd59d723a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.538641\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.024696\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000090\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_model = MS([poly('horsepower', 2, raw=True)])\n",
    "quad_func = partial(boot_OLS,\n",
    "quad_model,\n",
    "'mpg')\n",
    "boot_SE(quad_func, Auto, B=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a2374-dcb8-4684-bbad-9e009130c83c",
   "metadata": {},
   "source": [
    "We compare these results with the standard errors obtained from sm.OLS()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "428db924-e491-4285-abb7-99bad379e6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto['mpg'],\n",
    "quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())['std err']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
